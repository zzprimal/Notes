- Spin locks are only useful and should be used on multiprocessor systems, since if we use it on a uniprocessor system with only 1 core and the thread that hit the spinlock is forced to wait we are in deadlock since the point of spinlocks is that we won't have to wait long for the lock to get aqquired so there is no point in wasting overhead in sleeping and rescheduling, but since there is only 1 core using a spinlock means we don't rescedule the current thread for another time and keep executing a loop asking if the lock is freed when it can be never freed since we are stuck executing the loop

- Completions are an interface meant to solve the problem of one thread waiting for another thread to complete a task before continuing executing and is a better alternative to semaphores in doing this task because apparently a flaw in the implementation of semaphores for implementing this task that causes a race condition and was not changed since they are optimized for the "non-contention case" and a fix would [change that](https://lkml.indiana.edu/hypermail/linux/kernel/0107.3/0674.html) (apparently the race condition that was caused by the flaw in semaphores was where apparently once the thread that has aqquired the semaphore frees it with up and wakes up threads waiting for it apparently the freer still touches semaphore data structures and the thread that was woken up also does the same, atleast that's my interpretation of it, reading the source code of kernel semaphores is required for a deeper understanding)

- Once a spinlock is aqquired it is never a good idea to have the thread that aqquired said spinlock to sleep and give up the cpu, because in the worse case this can cause your system to stall completely if enough threads are waiting for said lock and if the thread that has the lock aqquired is not able to get cpu time in the forseeable future, so writing spinlock code for the critical section needs to be done with caution, kernel preemption is autimatically turned off when aqquiring spinlocks so that helps prevent some interrupts (apparently this doesn't prevent all interrupts, need to read source code), but there is still the possibility of calling functions that do sleep and give up the cpu in the critical section and this is what you need to pay attention to



- There are spinlock aqquiring functions that allow you to turn off hardware or software interrupts, depending on whether there exists interrupt handlers for either of these that tries to aqquire the same spinlock you much turn off use a lock aqquiring function that turns off these interrupts else you run the risk of completely deadlocking your processor

- For semaphores and for spinlocks there exists read write versions of these locks, meaning it can allow infinite readers to aqquire the lock at once but only 1 writer can have the lock at once with no other writers and readers, this increases performance since now readers don't have to wait for eachother to read a value that won't change, these types of locks will give precedence to writing locks so read locks can get most up to date values, since this is the case though if you have many writes you can potentially starve your read threads so this should only be used in cases where writing does not happen frequently

- Preferably you would prefer to access a shared resource with multiple threads without using locks or without full locking (meaning only 1 thread per critical section) since locks causes threads to have to wait and slow down either your program or take unecessary processing power (for semaphores and spinlocks respectively), a solution to this is using algorithms that allow multiple threads accessing shared resources but don't cause any race conditions, deadlocks, and all that bad stuff, an example are the previously mentioned read write version of semaphores and spin locks, but some others are as follows...
    - Read Copy Update (RCU) is one of these algorithms, the idea is that the shared resource is accessed completely through pointers and once a write needs to be done to a shared data structure the writing thread first reads the current values for said data structure, copies said values, then changes the pointers in the data structure to point to the new copy, and once all the references to the old copy have completely disappeared from all threads the old data structure will be freed (this last step requires specific previleges to be able to track all references to a pointer for all threads executing in kernel space, something like this can only be done via kernel privalages so this type of algorithm does not really work for user space processes), this type of algorithm only works for specific shared resources/data structures where swapping of pointers from old to new data works (something like a linked list for example) and also works best for when reads are common but writes are rare, but if there is the possibility for multiple concurrent writers a lock will be necessary for said writers
    - Seqlocks is another such algorithm, it works by having readers access the shared resource like normal, but when it is finished checks if there was any collisions with writers (meaning if there was a writer who accesses and updated the shared resource while the reader was using it), if such a situation occured then the reader will retry access to the shared resource again hoping not to get a collision with a writer (collision checking is done by associating a sequence number with the shared resource that is incremented by a writer on entering a critical section and exiting the critical section, readers will check upon entering if the sequence number is odd indicating a writer is currently updating, and at the end if the sequence number changed during executing the critical section indicating there was a update, both siutations will require a retry), this resource obviously works very well again when there are alot of readers but very little writers and allows for very fast writes (atleast for the standard of accessing shared resources) since threads can write at anytime and readers are forced to retry, but this algorithm cannot be used if the protected data contains pointers since it is possible that the writer can invalidate a pointer while a reader is reading, it would then be better to just use RCU instead which keeps all memory allocated to a pointer till there are no more references. A writing thread executing the critical section must never be preempted or interrupted by an interrupt handler that tries to read the same shared resource, else you will have the same situation with spinlocks where in this case it will notice the sequence number is odd and constantly spin (since seqlock uses spinlocks for retrying reads) and possibly deadlock your processor
    - Bitwise operations are another algorithm and in this case requires zero locking, these operations are inherritly atomic since they only requires only 1 step or instruction (might need more reasearch if this is true), so multiple threads executing these at the same time is invulnerable to race conditions
    - Atomic operations are another example, if your shared resource is just a simple integer you need to increment with multiple threads just simple atomic operations are sufficient and the linux kernel provides a library for this
    - Sometimes a data structure can be accessed by multiple threads without the need of any locks of any kind, an example would be circular buffers assuming you are allowed only 1 writing thread and 1 reading thread since writing and reading to a circular buffer are completely seperate from eachother

- Should read sections just be fetching the read data or also contain all operations on said data?

- Another interesting thread from the [linux kernel mailing list](https://yarchive.net/comp/linux/semaphores.html)