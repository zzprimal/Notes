- ioctl implements command numbers via 4 byte groups, the first byte group specifies a magic number which is unique for the driver, the second byte group specifies a ordinal number which is a unique number for the device file interacting with the driver, the third byte group specifies a direction of data transfer, if the ioctl operation is a read operation it will be _IOC_READ, if write it will be _IOC_WRITE, if both it will be _IOC_READ | _IOC_WRITE, if none it will be _IOC_NONE,
and the last byte group is the size of the data involved in the previously mentioned data transfer

-ioctl in userspace has 3 arguments, the fd for the device file, the ioctl command to specify the operation to be performed, and finally an argument that can be literally anything, it can be a pointer to userspace to be used as a return value, or it can be a non-pointer type that is just meant to give an argument to the ioctl call, no type checking is done on this final argument by the compiler since a hack involving variable amount of argument is done where the decalaration for ioctl in user space just specifies 2 type signature for the paramaters and then says there can be a variable amount of arguments when in reality it only uses the third argument, this is to avoid type checking like previously mentioned

- linux implements a capability functionality in the kernel which checks whether a user has the right permissions to perform a specific functionality or interact with drivers in a specific way, these need to be check specifically in the driver code to see if the user is allowed to perform a certain operation with this device

- another way to control a device without using io commands is with writing to the device control sequences using the write system call and a very specific sequence on inputs, one device that does this are ttys and escape sequences where escape sequences can control a multitude of things about the terminal, stuff like color of specific areas of the terminal, font, etc. The downside to this is that you have to make sure normal use of the device does not invoke these operations when unneeded or not intended. You can also do the opposite and instead of only using the write system call and not the ioctl system call you want to instead only use the ioctl system call and not the write system call, these approaches switches the complexity of the code for the driver and interacting with the driver from user space (from kernel and user space)

- blocking io for drivers can be implemented via creating a wait queue object and putting the process to sleep with the wait_event function using the wait queue as argument and also a condition where the process can only move on from sleeping if the condition evaluates to true, then a different process can wake up said process by calling the wake_up function and also setting the condition to be true before the wake up (else condition will evaluate to false on wake up and the process will just go back to sleep)

- all kernel drivers should implement a output buffer for write operations to the device since this uses less system resources, the reason for this is because without it every writing process will have to wait for a reading process to read the device, and if that reading process does not read all the data from the writing device it will then have to wait for another reading device, and another, and so on and so on, this causes a lot of context switches and switches between user and kernel space as opposed to just implementing a output buffer and having the writing process write all its data there so we will never have unnecessary context switches and switches between user and kernel space ever again

- open system call can be set with the argument O_NONBLOCK which means all open, read, and write operations are non-blocking regardless of whether or not the data is available

- wait queues in the linux kernel are implemented via a linked list and a spinlock, whenever a process needs to sleep for a task it first sets the state of the process to one of TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE to signify to the scheduler the process is now sleeping, but we still need to give up the cpu which is done via a call to schedule(), there is still a race condition here though which is what happens if the sleep condition becomes true after we start doing this process, then we will be woken up before we go to sleep at all and cause unnecessary and potentially long waiting or sleep forever if we never get a wakeup call, for this reason we check the state of condition before the schedule call and if it's true we revert these changes and go back to normal execution (check <linux/wait.h> and <linux/sched.h>)

- What happens if wakeup occurs inbetween conditional check and schedule call, in this case all is good since wakeup sets process back to TASK_RUNNING which causes schedule to return immediately, problems are only cost if we don't do the conditional check before schedule and wakeup occurs before setting process status to TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE

- Sometimes when wait queues are really long waking up all processes in the wait queue can waste resources since they all have to contend for resources anyways and will be put back to sleep, so we're wasting time on context switches for all these processes, exclusive wakes solve this problem where on wakeup once a process with exclusive wait flag is encountered it stops waking up the rest of the processes in the queue, this is good if there is potential to be mass contention for a resource and one process is enough to consume the resource (though you can also wake up n exclusive waiters aswell)

- to implement waits can be done via wait_events or manually each called simple and advanced sleeps respectively where manually doing it means you have to code the whole behaviour of your process up till it goes to sleep, if you want to use exclusive waits you have to use advanced sleeps

- the poll file operation must do 2 things, add all wait queues that are used to wake up processes to the wait structure, and return the flags that should be returned if the poll call is non-blocking

- polls are implemented by the poll file operation for a devices driver code, wait queues that are used to wake up processes on reads and writes should be put in a poll_table structure using the poll_wait function, this is used to wake up the process on read and write on poll block

- the step by step process how poll used to work for around the 2.5 version of the linux kernel is as follows, first poll calls the poll method for all files that are given to it via the array of structs passing a poll table to each which is just a wrapper around a function call to create the poll data structure which is a linked list of poll table entry structure, each poll table entry struct contains the corresponding file struct pointer, the wait queue that was added that corresponds with said file, and the associated wait queue entry for the process which is probably added to the wait during poll_wait. If none of the files indicates non-blocking io can be done (this is probably determined by using the return value of the poll method) we go into sleep until one of the drivers wakes up the process then we check i assume the poll table entry struct linked list to find the driver that woke us up and the associated file (some of this is speculation and im just piecing together missing pieces for how i think it's implemented using the explanation the textbook gives)

- epoll is a version of poll and select that is more compatible with many different files and devices being polled, this is because poll and selects tears down the linked list data structure on each call and if there are alot of files this is alot of overhead, epoll does not do this teardown and keeps it during each system call

- asynchronous notifications is another method to do non-blocking io on a file, it works by setting the F_SETOWN flag for a file which save the process id of the caller into filp->f_owner which the kernel uses later to know who to notify, you can then set asynchronous notifications by setting the FASYNC flag on the file, both these flags can be set using fcntl and the kernel alerts the process by sending the SIGIO signal to it so you have to set a signal handler, the problem with this is you can't determine which file is sending the signal and hence which file has io ready, this is better suited for a situation for example where you don't want your program to block at all and want it to execute different code in the meantime but still want to know when io is available for a specific file or device immediately, also this has to be supported by the driver in question for said device/file

- implementing exclusive access to your device to only 1 process at a time or 1 user at a time can also be done, the former can be done by just implementing a non-blocking lock on the device and if the lock is occupied deny access in open, the later can be implemented by keeping track of the UID of the process that aqquires the device lock, or if you want you can block the process until the device is free, this can be done using the previously mentioned methods in the textbook using waits and wait queues

- another driver implementation you can have is have a new device be allocated for each user or process that interacts with the driver code, so on open if you encounter new user or process you would malloc space in kernel space for another device and unique device said new user or process can interact with

